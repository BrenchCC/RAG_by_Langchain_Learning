{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 首先创建一个chat llm without rag",
   "id": "609dbeec105a5503"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T19:53:22.111896Z",
     "start_time": "2025-07-05T19:53:22.082525Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "chatLLM = ChatOpenAI(\n",
    "    api_key = \"sk-1c33a0011b5c4ef6ac842d58255a20d8\",\n",
    "    base_url = \"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
    "    model = \"qwen2-7b-instruct\",  # 此处为了演示幻觉，以qwen2-7b-instruct为例，您可按需更换模型名称。模型列表：https://help.aliyun.com/zh/model-studio/getting-started/models\n",
    "    # other params...\n",
    ")"
   ],
   "id": "dd97f9bd74219e65",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T19:53:24.885635Z",
     "start_time": "2025-07-05T19:53:22.883347Z"
    }
   },
   "cell_type": "code",
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"你是谁？\"}]\n",
    "response = chatLLM.invoke(messages)\n",
    "print(response)\n",
    "print(response.content)"
   ],
   "id": "9bb1a69600633dec",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='我是阿里云开发的一款超大规模语言模型，我叫通义千问。作为一个AI助手，我的目标是帮助用户获得准确、有用的信息，解决他们的问题和困惑。无论是关于科技、教育、生活常识还是其他领域的问题，我都会尽力提供最合适的帮助。请随时向我提问，我会努力回答。' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 69, 'prompt_tokens': 22, 'total_tokens': 91, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'qwen2-7b-instruct', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run--9b0cdda3-2c0d-44cb-9d31-9c260f34f86f-0'\n",
      "我是阿里云开发的一款超大规模语言模型，我叫通义千问。作为一个AI助手，我的目标是帮助用户获得准确、有用的信息，解决他们的问题和困惑。无论是关于科技、教育、生活常识还是其他领域的问题，我都会尽力提供最合适的帮助。请随时向我提问，我会努力回答。\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 处理LLM存在的缺陷问题\n",
    "\n",
    "- 容易出现幻觉\n",
    "- 数据信息时效性滞后\n",
    "- 专业领域深度知识匮乏"
   ],
   "id": "893612a2b5a18cf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T19:53:25.864360Z",
     "start_time": "2025-07-05T19:53:25.860729Z"
    }
   },
   "cell_type": "code",
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"你知道baichuan2模型吗？\"}\n",
    "]"
   ],
   "id": "49a5bdc7dcb7938c",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T19:53:30.698873Z",
     "start_time": "2025-07-05T19:53:27.163116Z"
    }
   },
   "cell_type": "code",
   "source": [
    "response = chatLLM.invoke(messages)\n",
    "print(response.content)"
   ],
   "id": "e6a114bb3a9bf8c3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "是的，我知道Baichuan 2。Baichuan 2是由阿里云开发的一个大型语言模型，全名是“通义千问”。它是基于阿里云的超大规模神经网络训练而成，旨在通过学习大量的文本数据来生成与之类似的文字内容。相较于其前身，Baichuan 2在性能、准确性和生成质量上都有了显著提升。\n",
      "\n",
      "作为阿里云的AI助手，通义千问可以回答各种问题、提供信息、进行对话，并且能够根据不同的场景和需求生成相应的文本内容。例如，它可以用于编写故事、解答技术问题、提供商业策略建议等。它具备一定的语言理解能力，能够对用户的输入进行分析，并基于此提供相关回复或执行相应的任务。\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T19:53:59.277746Z",
     "start_time": "2025-07-05T19:53:59.276006Z"
    }
   },
   "cell_type": "code",
   "source": [
    "baichuan2_information = [\n",
    "    \"Baichuan 2是一个大规模多语言语言模型，它专注于训练在多种语言中表现优异的模型，包括不仅限于英文。这使得Baichuan 2在处理各种语言的任务时能够取得显著的性能提升。\",\n",
    "    \"Baichuan 2是从头开始训练的，使用了包括了2.6万亿个标记的庞大训练数据集。相对于以往的模型，Baichuan 2提供了更丰富的数据资源，从而能够更好地支持多语言的开发和应用。\",\n",
    "    \"Baichuan 2不仅在通用任务上表现出色，还在特定领域（如医学和法律）的任务中展现了卓越的性能。这为特定领域的应用提供了强有力的支持。\"\n",
    "]\n",
    "\n",
    "source_knowledge = \"\\n\".join(baichuan2_information)"
   ],
   "id": "2bf1a9807df5817e",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T19:54:15.450909Z",
     "start_time": "2025-07-05T19:54:15.445724Z"
    }
   },
   "cell_type": "code",
   "source": "source_knowledge",
   "id": "149b3e31c2c5db8e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Baichuan 2是一个大规模多语言语言模型，它专注于训练在多种语言中表现优异的模型，包括不仅限于英文。这使得Baichuan 2在处理各种语言的任务时能够取得显著的性能提升。\\nBaichuan 2是从头开始训练的，使用了包括了2.6万亿个标记的庞大训练数据集。相对于以往的模型，Baichuan 2提供了更丰富的数据资源，从而能够更好地支持多语言的开发和应用。\\nBaichuan 2不仅在通用任务上表现出色，还在特定领域（如医学和法律）的任务中展现了卓越的性能。这为特定领域的应用提供了强有力的支持。'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T19:55:27.525604Z",
     "start_time": "2025-07-05T19:55:27.523318Z"
    }
   },
   "cell_type": "code",
   "source": [
    "query = \"你知道baichuan2模型吗？\"\n",
    "\n",
    "prompt_template = f\"\"\"基于以下内容回答问题：\n",
    "\n",
    "内容:\n",
    "{source_knowledge}\n",
    "\n",
    "Query: {query}\"\"\""
   ],
   "id": "5b718403686cb842",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T19:55:28.112628Z",
     "start_time": "2025-07-05T19:55:28.110512Z"
    }
   },
   "cell_type": "code",
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt_template}\n",
    "]"
   ],
   "id": "220aba207ec9c063",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T19:55:35.995059Z",
     "start_time": "2025-07-05T19:55:33.263479Z"
    }
   },
   "cell_type": "code",
   "source": [
    "response = chatLLM.invoke(messages)\n",
    "print(response.content)"
   ],
   "id": "2c7d752148d1547",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baichuan 2是一个大型多语言语言模型，专注于训练在多种语言中表现优异的模型，包括但不限于英文。它从头开始训练，使用了包含2.6万亿个标记的大量训练数据集，相比以往的模型提供了更丰富的数据资源，从而更好地支持多语言的开发和应用。Baichuan 2在通用任务和特定领域（如医学和法律）的任务中都表现出色，为特定领域的应用提供了有力支持。\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T19:58:09.856167Z",
     "start_time": "2025-07-05T19:58:06.814837Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 加载pdf数据\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"https://arxiv.org/pdf/2309.10305v2.pdf\")\n",
    "# loader = PyPDFLoader(\"./pdf/2309.10305v2.pdf\")\n",
    "pages = loader.load_and_split()"
   ],
   "id": "1dc8f83b3500d03b",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T19:58:21.265994Z",
     "start_time": "2025-07-05T19:58:21.263266Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(len(pages))\n",
    "pages[0]"
   ],
   "id": "be42f40438215192",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-09-21T00:15:31+00:00', 'author': '', 'keywords': '', 'moddate': '2023-09-21T00:15:31+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2309.10305v2.pdf', 'total_pages': 28, 'page': 0, 'page_label': '1'}, page_content='Baichuan 2: Open Large-scale Language Models\\nAiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Chao Yin, Chenxu Lv, Da Pan\\nDian Wang, Dong Yan, Fan Yang, Fei Deng, Feng Wang, Feng Liu, Guangwei Ai\\nGuosheng Dong, Haizhou Zhao, Hang Xu, Haoze Sun, Hongda Zhang, Hui Liu, Jiaming Ji\\nJian Xie, Juntao Dai, Kun Fang, Lei Su, Liang Song, Lifeng Liu, Liyun Ru, Luyao Ma\\nMang Wang, Mickel Liu, MingAn Lin, Nuolan Nie, Peidong Guo, Ruiyang Sun\\nTao Zhang, Tianpeng Li, Tianyu Li, Wei Cheng, Weipeng Chen, Xiangrong Zeng\\nXiaochuan Wang, Xiaoxi Chen, Xin Men, Xin Yu, Xuehai Pan, Yanjun Shen, Yiding Wang\\nYiyu Li, Youxin Jiang, Yuchen Gao, Yupeng Zhang, Zenan Zhou, Zhiying Wu\\nBaichuan Inc.\\nAbstract\\nLarge language models (LLMs) have\\ndemonstrated remarkable performance on\\na variety of natural language tasks based\\non just a few examples of natural language\\ninstructions, reducing the need for extensive\\nfeature engineering. However, most powerful\\nLLMs are closed-source or limited in their\\ncapability for languages other than English. In\\nthis technical report, we present Baichuan 2,\\na series of large-scale multilingual language\\nmodels containing 7 billion and 13 billion\\nparameters, trained from scratch, on 2.6 trillion\\ntokens. Baichuan 2 matches or outperforms\\nother open-source models of similar size on\\npublic benchmarks like MMLU, CMMLU,\\nGSM8K, and HumanEval. Furthermore,\\nBaichuan 2 excels in vertical domains such\\nas medicine and law. We will release all\\npre-training model checkpoints to benefit the\\nresearch community in better understanding\\nthe training dynamics of Baichuan 2.\\n1 Introduction\\nThe field of large language models has witnessed\\npromising and remarkable progress in recent years.\\nThe size of language models has grown from\\nmillions of parameters, such as ELMo (Peters\\net al., 2018), GPT-1 (Radford et al., 2018), to\\nbillions or even trillions of parameters such as GPT-\\n3 (Brown et al., 2020), PaLM (Chowdhery et al.,\\n2022; Anil et al., 2023) and Switch Transformers\\n(Fedus et al., 2022). This increase in scale has\\nled to significant improvements in the capabilities\\nof language models, enabling more human-like\\nfluency and the ability to perform a diverse range\\nof natural language tasks. With the introduction of\\nAuthors are listed alphabetically, correspondent:\\ndaniel@baichuan-inc.com.\\nChatGPT (OpenAI, 2022) from OpenAI, the power\\nof these models to generate human-like text has\\ncaptured widespread public attention. ChatGPT\\ndemonstrates strong language proficiency across\\na variety of domains, from conversing casually to\\nexplaining complex concepts. This breakthrough\\nhighlights the potential for large language models\\nto automate tasks involving natural language\\ngeneration and comprehension.\\nWhile there have been exciting breakthroughs\\nand applications of LLMs, most leading LLMs like\\nGPT-4 (OpenAI, 2023), PaLM-2 (Anil et al., 2023),\\nand Claude (Claude, 2023) remain closed-sourced.\\nDevelopers and researchers have limited access to\\nthe full model parameters, making it difficult for\\nthe community to deeply study or fine-tune these\\nsystems. More openness and transparency around\\nLLMs could accelerate research and responsible\\ndevelopment within this rapidly advancing field.\\nLLaMA (Touvron et al., 2023a), a series of large\\nlanguage models developed by Meta containing up\\nto 65 billion parameters, has significantly benefited\\nthe LLM research community by being fully open-\\nsourced. The open nature of LLaMA, along with\\nother open-source LLMs such as OPT (Zhang\\net al., 2022), Bloom (Scao et al., 2022), MPT\\n(MosaicML, 2023) and Falcon (Penedo et al.,\\n2023), enables researchers to freely access the\\nmodels for examination, experimentation, and\\nfurther development. This transparency and access\\ndistinguishes LLaMA from other proprietary\\nLLMs. By providing full access, the open-source\\nLLMs have accelerated research and advances in\\nthe field, leading to new models like Alpaca (Taori\\net al., 2023), Vicuna (Chiang et al., 2023), and')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T19:59:13.452335Z",
     "start_time": "2025-07-05T19:59:13.445602Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 知识切片 将文档分割成均匀的块。每个块是一段原始文本\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 500,\n",
    "    chunk_overlap = 50,\n",
    ")\n",
    "\n",
    "docs = text_splitter.split_documents(pages)\n",
    "len(docs)"
   ],
   "id": "1b8d3087238c7bfd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "216"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T20:18:08.659112Z",
     "start_time": "2025-07-05T20:17:55.893446Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.embeddings import DashScopeEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "embed_model = DashScopeEmbeddings(\n",
    "    model = \"text-embedding-v2\",\n",
    "    dashscope_api_key = \"sk-1c33a0011b5c4ef6ac842d58255a20d8\"\n",
    ")\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=docs,\n",
    "    embedding=embed_model,\n",
    "    collection_name=\"brench_embed_dev\",\n",
    "    persist_directory=\"./chroma_data_baichuan2_rag_demo\"\n",
    ")"
   ],
   "id": "16304e4c00a90659",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T20:21:52.665535Z",
     "start_time": "2025-07-05T20:21:52.366703Z"
    }
   },
   "cell_type": "code",
   "source": [
    "query = \"百川2大模型是什么，以及百川2大模型用的参数大小?\"\n",
    "result = vectorstore.similarity_search(query ,k = 2)\n",
    "result"
   ],
   "id": "abd832c7b4559b1f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'page_label': '2', 'subject': '', 'creationdate': '2023-09-21T00:15:31+00:00', 'creator': 'LaTeX with hyperref', 'moddate': '2023-09-21T00:15:31+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'total_pages': 28, 'author': '', 'source': 'https://arxiv.org/pdf/2309.10305v2.pdf', 'producer': 'pdfTeX-1.40.25', 'trapped': '/False', 'title': '', 'keywords': '', 'page': 1}, page_content='languages, such as Chinese.\\nIn this technical report, we introduce Baichuan\\n2, a series of large-scale multilingual language\\nmodels. Baichuan 2 has two separate models,\\nBaichuan 2-7B with 7 billion parameters and\\nBaichuan 2-13B with 13 billion parameters. Both\\nmodels were trained on 2.6 trillion tokens, which\\nto our knowledge is the largest to date, more than\\ndouble that of Baichuan 1 (Baichuan, 2023b,a).\\nWith such a massive amount of training data,'),\n",
       " Document(metadata={'creationdate': '2023-09-21T00:15:31+00:00', 'title': '', 'trapped': '/False', 'page': 1, 'subject': '', 'producer': 'pdfTeX-1.40.25', 'total_pages': 28, 'author': '', 'source': 'https://arxiv.org/pdf/2309.10305v2.pdf', 'moddate': '2023-09-21T00:15:31+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'keywords': '', 'page_label': '2', 'creator': 'LaTeX with hyperref'}, page_content='overall performance of the Baichuan 2 base models\\ncompared to other open or closed-sourced models')]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T20:21:53.107127Z",
     "start_time": "2025-07-05T20:21:53.104979Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def augment_prompt(query: str):\n",
    "  # 获取top3的文本片段\n",
    "  results = vectorstore.similarity_search(query, k=3)\n",
    "  source_knowledge = \"\\n\".join([x.page_content for x in results])\n",
    "  # 构建prompt\n",
    "  augmented_prompt = f\"\"\"Using the contexts below, answer the query.\n",
    "\n",
    "  contexts:\n",
    "  {source_knowledge}\n",
    "\n",
    "  query: {query}\"\"\"\n",
    "  return augmented_prompt"
   ],
   "id": "9cf9da71f2c67285",
   "outputs": [],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T20:21:53.866315Z",
     "start_time": "2025-07-05T20:21:53.610986Z"
    }
   },
   "cell_type": "code",
   "source": "print(augment_prompt(query))",
   "id": "5c16628b41452f03",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the contexts below, answer the query.\n",
      "\n",
      "  contexts:\n",
      "  languages, such as Chinese.\n",
      "In this technical report, we introduce Baichuan\n",
      "2, a series of large-scale multilingual language\n",
      "models. Baichuan 2 has two separate models,\n",
      "Baichuan 2-7B with 7 billion parameters and\n",
      "Baichuan 2-13B with 13 billion parameters. Both\n",
      "models were trained on 2.6 trillion tokens, which\n",
      "to our knowledge is the largest to date, more than\n",
      "double that of Baichuan 1 (Baichuan, 2023b,a).\n",
      "With such a massive amount of training data,\n",
      "overall performance of the Baichuan 2 base models\n",
      "compared to other open or closed-sourced models\n",
      "A Scaling laws\n",
      "We use 7 models to fit the scaling laws of Baichuan\n",
      "2. The parameter details are shown in Table 10.\n",
      "Nhidden NFFN Nlayer Nhead Nparams (Millions)\n",
      "384 1,152 6 6 11.51\n",
      "704 2,112 8 8 51.56\n",
      "832 2,496 12 8 108.01\n",
      "1,216 3,648 16 8 307.60\n",
      "1,792 5,376 20 14 835.00\n",
      "2,240 6,720 24 14 1,565.60\n",
      "2,880 8,640 28 20 3,019.33\n",
      "Table 10: The model we choose for fitting scaling laws.\n",
      "The losses of the 7 different models are shown\n",
      "in Figure 8.\n",
      "Figure 8: The various training loss of small models for\n",
      "\n",
      "  query: 百川2大模型是什么，以及百川2大模型用的参数大小?\n"
     ]
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T20:21:54.765437Z",
     "start_time": "2025-07-05T20:21:54.440265Z"
    }
   },
   "cell_type": "code",
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": augment_prompt(query)}\n",
    "]"
   ],
   "id": "5c241f8ed00757bf",
   "outputs": [],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T20:22:06.517023Z",
     "start_time": "2025-07-05T20:21:55.466352Z"
    }
   },
   "cell_type": "code",
   "source": [
    "response = chatLLM.invoke(messages)\n",
    "print(response.content)"
   ],
   "id": "6d4e43a6e0440048",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "百川2是多语言大规模语言模型系列的一种。它包含了两个独立的模型：百川2-7B，拥有70亿个参数；和百川2-13B，拥有130亿个参数。这两个模型都是在26万亿个标记上进行训练的，据我们所知，这是迄今为止最大的数据集，超过Baichuan 1的两倍（Baichuan, 2023b,a）。通过如此大量的训练数据，百川2基模的整体性能与其他开源或闭源模型相比有了显著提升。\n",
      "\n",
      "在研究中使用了7个模型来拟合百川2的扩展法则。这些模型的详细参数如表10所示：\n",
      "\n",
      "| 隐藏层大小 | FFN（前馈神经网络）数量 | 层数 | 头数 | 参数（百万） |\n",
      "| --- | --- | --- | --- | --- |\n",
      "| 384 | 1,152 | 6 | 6 | 11.51 |\n",
      "| 704 | 2,112 | 8 | 8 | 51.56 |\n",
      "| 832 | 2,496 | 12 | 8 | 108.01 |\n",
      "| 1,216 | 3,648 | 16 | 8 | 307.60 |\n",
      "| 1,792 | 5,376 | 20 | 14 | 835.00 |\n",
      "| 2,240 | 6,720 | 24 | 14 | 1,565.60 |\n",
      "| 2,880 | 8,640 | 28 | 20 | 3,019.33 |\n",
      "\n",
      "表10展示了用于拟合扩展法则的模型选择。不同模型的损失值在图8中有所显示。\n",
      "\n",
      "因此，百川2大模型是指百川2-7B（70亿参数）和百川2-13B（130亿参数）两种模型。\n"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c0839d2e2be66ba3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
